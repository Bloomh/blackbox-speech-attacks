{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "# Directory containing batch attack results\n",
    "results_dir = 'batch_attack_results'\n",
    "\n",
    "# 1. List all summary and per-run CSVs\n",
    "all_csvs = glob.glob(os.path.join(results_dir, '*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all of the per_run_csvs into a single dataframe\n",
    "dfs = [pd.read_csv(f) for f in all_csvs]\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Optionally remove any normal0.wav data\n",
    "df_all = df_all[df_all['input_wav'] != 'processed_sound/normal0.wav']\n",
    "\n",
    "# Convert strings to lists\n",
    "for col in ['ensemble_losses', 'target_losses', 'ensemble_lev_dists', 'target_lev_dists']:\n",
    "    df_all[col] = df_all[col].apply(ast.literal_eval)\n",
    "\n",
    "print(df_all.head())\n",
    "print(f\"Combined DataFrame shape: {df_all.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Target Loss ---\n",
    "loss_lists = df_all[\"target_losses\"].dropna()\n",
    "max_len = max(len(lst) for lst in loss_lists)\n",
    "loss_array = np.full((len(loss_lists), max_len), np.nan)\n",
    "for i, lst in enumerate(loss_lists):\n",
    "    loss_array[i, :len(lst)] = lst\n",
    "mean_target_loss = np.nanmean(loss_array, axis=0)\n",
    "\n",
    "# --- Ensemble Loss ---\n",
    "ensemble_loss_lists = df_all[\"ensemble_losses\"].dropna()\n",
    "# Flatten to shape: (runs, ensemble_models, steps)\n",
    "max_ens = max(len(run) for run in ensemble_loss_lists)\n",
    "max_steps = max(len(model) for run in ensemble_loss_lists for model in run)\n",
    "ens_loss_array = np.full((len(ensemble_loss_lists), max_ens, max_steps), np.nan)\n",
    "for i, run in enumerate(ensemble_loss_lists):\n",
    "    for j, model in enumerate(run):\n",
    "        ens_loss_array[i, j, :len(model)] = model\n",
    "# Mean over runs and ensemble models\n",
    "mean_ensemble_loss = np.nanmean(ens_loss_array, axis=(0,1))\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(mean_target_loss, label='Target')\n",
    "plt.plot(mean_ensemble_loss, label='Ensemble')\n",
    "plt.xlabel('PGD Iteration')\n",
    "plt.ylabel('Mean Loss')\n",
    "plt.title('Mean Loss at Each PGD Iteration (Target vs Ensemble)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Target Lev Dist ---\n",
    "target_lev_dists = df_all[\"target_lev_dists\"].dropna()\n",
    "max_len = max(len(lst) for lst in target_lev_dists)\n",
    "target_lev_array = np.full((len(target_lev_dists), max_len), np.nan)\n",
    "for i, lst in enumerate(target_lev_dists):\n",
    "    target_lev_array[i, :len(lst)] = lst\n",
    "mean_target_lev = np.nanmean(target_lev_array, axis=0)\n",
    "\n",
    "# --- Ensemble Lev Dist ---\n",
    "ensemble_lev_dists = df_all[\"ensemble_lev_dists\"].dropna()\n",
    "# Flatten to shape: (runs, ensemble_models, steps)\n",
    "max_ens = max(len(run) for run in ensemble_lev_dists)\n",
    "max_steps = max(len(model) for run in ensemble_lev_dists for model in run)\n",
    "ens_lev_array = np.full((len(ensemble_lev_dists), max_ens, max_steps), np.nan)\n",
    "for i, run in enumerate(ensemble_lev_dists):\n",
    "    for j, model in enumerate(run):\n",
    "        ens_lev_array[i, j, :len(model)] = model\n",
    "# Mean over runs and ensemble models\n",
    "mean_ensemble_lev = np.nanmean(ens_lev_array, axis=(0,1))\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(mean_target_lev, label='Mean Target Lev Dist')\n",
    "plt.plot(mean_ensemble_lev, label='Mean Ensemble Lev Dist')\n",
    "plt.xlabel('PGD Iteration')\n",
    "plt.ylabel('Mean Lev Dist')\n",
    "plt.title('Mean Lev Dist at Each PGD Iteration (Target vs Ensemble)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Target Lev Dist ---\n",
    "target_lev_dists = df_all[\"target_lev_dists\"].dropna()\n",
    "max_len = max(len(lst) for lst in target_lev_dists)\n",
    "target_lev_array = np.full((len(target_lev_dists), max_len), np.nan)\n",
    "for i, lst in enumerate(target_lev_dists):\n",
    "    target_lev_array[i, :len(lst)] = lst\n",
    "mean_target_lev = np.nanmean(target_lev_array, axis=0)\n",
    "std_target_lev = np.nanstd(target_lev_array, axis=0)\n",
    "\n",
    "# --- Ensemble Lev Dist ---\n",
    "ensemble_lev_dists = df_all[\"ensemble_lev_dists\"].dropna()\n",
    "max_ens = max(len(run) for run in ensemble_lev_dists)\n",
    "max_steps = max(len(model) for run in ensemble_lev_dists for model in run)\n",
    "ens_lev_array = np.full((len(ensemble_lev_dists), max_ens, max_steps), np.nan)\n",
    "for i, run in enumerate(ensemble_lev_dists):\n",
    "    for j, model in enumerate(run):\n",
    "        ens_lev_array[i, j, :len(model)] = model\n",
    "mean_ensemble_lev = np.nanmean(ens_lev_array, axis=(0,1))\n",
    "std_ensemble_lev = np.nanstd(ens_lev_array, axis=(0,1))\n",
    "\n",
    "# --- Plot ---\n",
    "k = 0.5  # Change this factor as desired\n",
    "x = np.arange(len(mean_target_lev))\n",
    "x_ens = np.arange(len(mean_ensemble_lev))\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(mean_target_lev, label='Target')\n",
    "plt.fill_between(x, mean_target_lev - k*std_target_lev, mean_target_lev + k*std_target_lev, alpha=0.2)\n",
    "\n",
    "plt.plot(mean_ensemble_lev, label='Ensemble')\n",
    "plt.fill_between(x_ens, mean_ensemble_lev - k*std_ensemble_lev, mean_ensemble_lev + k*std_ensemble_lev, alpha=0.2)\n",
    "\n",
    "plt.xlabel('PGD Iteration')\n",
    "plt.ylabel('Mean Levenshtein Distance')\n",
    "plt.title('Mean Levenshtein Distance at Each PGD Iteration (Target vs Ensemble) with 0.5 std')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First parse out the average of the last five levenshtein distances for the target\n",
    "df_all['final_target_lev_dist'] = df_all['target_lev_dists'].apply(lambda x: np.mean(x[-5:]))\n",
    "\n",
    "# Histogram of the final target levenshtein distances\n",
    "df_all['final_target_lev_dist'].hist(bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all of the last five Levenshtein distances from each entry\n",
    "last_five_lev_dists = df_all['target_lev_dists'].apply(lambda x: x[-10:])\n",
    "# Flatten the list of lists into a single list\n",
    "all_last_five = [item for sublist in last_five_lev_dists for item in sublist]\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(all_last_five, bins=10, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Target Levenshtein Distance (last 10 iterations)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of All Final 10 Target Levenshtein Distances')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all of the last ten Levenshtein distances from each entry\n",
    "last_ten_lev_dists = df_all['target_lev_dists'].apply(lambda x: x[-10:])\n",
    "all_last_ten = [item for sublist in last_ten_lev_dists for item in sublist]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.kdeplot(all_last_ten, fill=True, color='skyblue', linewidth=2)\n",
    "plt.xlabel('Target Levenshtein Distance (last 10 iterations)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Smoothed Distribution of All Final 10 Target Levenshtein Distances')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all of the last ten Levenshtein distances from each entry\n",
    "last_ten_lev_dists = df_all['target_lev_dists'].apply(lambda x: x[-10:])\n",
    "all_last_ten = [item for sublist in last_ten_lev_dists for item in sublist]\n",
    "\n",
    "# Calculate the mean initial Levenshtein distance (first entry of each sequence)\n",
    "initial_lev_dists = df_all['target_lev_dists'].apply(lambda x: x[0])\n",
    "mean_initial_lev_dist = np.mean(initial_lev_dists)\n",
    "\n",
    "# Calculate the mean of all last ten Levenshtein distances (across all entries)\n",
    "mean_last_ten_lev_dist = np.mean(all_last_ten)\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "sns.kdeplot(all_last_ten, fill=True, color='skyblue', linewidth=2)\n",
    "plt.axvline(mean_initial_lev_dist, color='red', linestyle='--', linewidth=2, label=f'Mean Initial Target Distance = {mean_initial_lev_dist:.2f}')\n",
    "plt.axvline(mean_last_ten_lev_dist, color='green', linestyle='--', linewidth=2, label=f'Mean of Last 10 Target Distances = {mean_last_ten_lev_dist:.2f}')\n",
    "plt.xlabel('Target Levenshtein Distance')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Smoothed Distribution of The Final 10 Target Levenshtein Distances')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_groupby(groupby_column, eval_column):\n",
    "    grouped = df_all.groupby(groupby_column)\n",
    "    n_groups = len(grouped)\n",
    "    colors = sns.color_palette('tab10', n_colors=n_groups)\n",
    "\n",
    "    # Collect all first and last ten distances for global x-axis range\n",
    "    all_first_ten = []\n",
    "    all_last_ten = []\n",
    "    for _, group in grouped:\n",
    "        first_ten_lev_dists = group[eval_column].apply(lambda x: x[:10])\n",
    "        last_ten_lev_dists = group[eval_column].apply(lambda x: x[-10:])\n",
    "        all_first_ten.extend([item for sublist in first_ten_lev_dists for item in sublist])\n",
    "        all_last_ten.extend([item for sublist in last_ten_lev_dists for item in sublist])\n",
    "\n",
    "    # Determine global min and max for x-axis\n",
    "    global_min = min(min(all_first_ten), min(all_last_ten))\n",
    "    global_max = max(max(all_first_ten), max(all_last_ten))\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 10), sharey=True)\n",
    "\n",
    "    # Panel 1: First 10\n",
    "    handles_1, labels_1, means_1 = [], [], []\n",
    "    for (target_val, group), color in zip(grouped, colors):\n",
    "        first_ten_lev_dists = group[eval_column].apply(lambda x: x[:10])\n",
    "        group_first_ten = [item for sublist in first_ten_lev_dists for item in sublist]\n",
    "        if len(group_first_ten) > 1:\n",
    "            mean_first = np.mean(group_first_ten)\n",
    "            label = f\"{target_val} (mean={mean_first:.2f})\"\n",
    "            line = axes[0].plot([], [], label=label)[0]  # dummy for handle\n",
    "            sns.kdeplot(group_first_ten, fill=True, color=color, linewidth=2, ax=axes[0], clip=(global_min, global_max))\n",
    "            handles_1.append(line)\n",
    "            labels_1.append(label)\n",
    "            means_1.append(mean_first)\n",
    "    # Sort by mean\n",
    "    sorted_1 = sorted(zip(means_1, handles_1, labels_1), key=lambda x: x[0])\n",
    "    handles_1_sorted = [h for _, h, _ in sorted_1]\n",
    "    labels_1_sorted = [l for _, _, l in sorted_1]\n",
    "    axes[0].set_xlim(global_min, global_max)\n",
    "    axes[0].set_title(f'First 10 {eval_column} by {groupby_column}')\n",
    "    axes[0].set_xlabel(f'{eval_column}')\n",
    "    axes[0].set_ylabel('Density')\n",
    "    axes[0].grid(axis='y', alpha=0.75)\n",
    "    axes[0].legend(handles=handles_1_sorted, labels=labels_1_sorted)\n",
    "\n",
    "    # Panel 2: Last 10\n",
    "    handles_2, labels_2, means_2 = [], [], []\n",
    "    for (target_val, group), color in zip(grouped, colors):\n",
    "        last_ten_lev_dists = group[eval_column].apply(lambda x: x[-10:])\n",
    "        group_last_ten = [item for sublist in last_ten_lev_dists for item in sublist]\n",
    "        if len(group_last_ten) > 1:\n",
    "            mean_last = np.mean(group_last_ten)\n",
    "            label = f\"{target_val} (mean={mean_last:.2f})\"\n",
    "            line = axes[1].plot([], [], label=label)[0]  # dummy for handle\n",
    "            sns.kdeplot(group_last_ten, fill=True, color=color, linewidth=2, ax=axes[1], clip=(global_min, global_max))\n",
    "            handles_2.append(line)\n",
    "            labels_2.append(label)\n",
    "            means_2.append(mean_last)\n",
    "    # Sort by mean\n",
    "    sorted_2 = sorted(zip(means_2, handles_2, labels_2), key=lambda x: x[0])\n",
    "    handles_2_sorted = [h for _, h, _ in sorted_2]\n",
    "    labels_2_sorted = [l for _, _, l in sorted_2]\n",
    "    axes[1].set_xlim(global_min, global_max)\n",
    "    axes[1].set_title(f'Last 10 {eval_column} by {groupby_column}')\n",
    "    axes[1].set_xlabel(f'{eval_column}')\n",
    "    axes[1].grid(axis='y', alpha=0.75)\n",
    "    axes[1].legend(handles=handles_2_sorted, labels=labels_2_sorted)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for groupby_column in ('target_version', 'target_model', 'input_wav', 'target_sentence'):\n",
    "    plot_with_groupby(groupby_column, 'target_lev_dists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for groupby_column in ('target_version', 'target_model', 'input_wav', 'target_sentence'):\n",
    "    plot_with_groupby(groupby_column, 'target_losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Build the contingency table\n",
    "contingency_table = pd.crosstab(df_all['input_wav'], df_all['target_model'])\n",
    "\n",
    "# Perform the Chi-squared test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(f\"Chi-squared statistic: {chi2:.2f}\")\n",
    "print(f\"p-value: {p:.4f}\")\n",
    "\n",
    "if p < 0.05:\n",
    "    print(\"There is a statistically significant association between input_wav and target_model.\")\n",
    "else:\n",
    "    print(\"No statistically significant association was found; any apparent link is likely due to random chance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
